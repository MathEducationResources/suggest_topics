{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the topic of a Math Question on Math Education Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **Machine Learning** to predict the topic of a Math Question from the [Math Education Resources](http://math-education-resources.com). For simplicity we will only consider two topics. Using [multiclass classification](https://en.wikipedia.org/wiki/Multiclass_classification) this can be extended to more than two topics (at the time of writing, April 2015, we have about 1500 questions with 150 topics on MER)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do:\n",
    "1. Clean up the code (move helper functions to helper.py) - Bernhard\n",
    "2. Fix pca; get feature importance - Alex\n",
    "3. Write convenience functions:\n",
    "  1. text -> topic\n",
    "  2. text -> list of most similar questions (k-nn / cosine dist)  - Alex\n",
    "4. Add the suggested topics to the database for questions w/o a topic\n",
    "5. Re-write code for parent topics - Bernhard\n",
    "6. Re-write train test split to:  - Alex\n",
    "    1. get at least one question from each topic\n",
    "    2. pick them with diff probabilities\n",
    "\n",
    "-----------------------\n",
    "For later:\n",
    "7. Add additional features (course, etc.)\n",
    "    1. graph them\n",
    "8. Put up recommendations on the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import helpers\n",
    "from pymongo import MongoClient\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "# machine learning modules\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# data preprocessing modules\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL:\n",
      "## - Topic\n",
      "==========\n",
      " 2 - Linear approximation\n",
      " 3 - Power iteration\n",
      " 1 - Alternating series test\n",
      " 4 - Cumulative distribution function\n",
      " 2 - Implicit differentiation\n",
      " 5 - Matrix diagonalization\n",
      " 1 - Determinants\n",
      " 1 - Center of mass\n",
      " 2 - Partial fractions\n",
      " 1 - Matrix operations\n",
      " 1 - Initial value problem\n",
      " 2 - Random walks\n",
      "37 - Substitution\n",
      " 4 - Expected value and median\n",
      " 4 - Trigonometric integral\n",
      " 2 - Rank and nullity\n",
      " 2 - Reflection\n",
      "50 - Taylor series\n",
      "45 - Eigenvalues and eigenvectors\n",
      " 8 - Integration by parts\n",
      " 1 - Matrix similarity\n",
      " 2 - Fundamental theorem of calculus\n",
      " 1 - Linear transformation definition\n",
      "39 - Probability density function\n"
     ]
    }
   ],
   "source": [
    "# create an array of all topics of interest\n",
    "topic_tags = [\"Eigenvalues_and_eigenvectors\",\n",
    "              \"Probability_density_function\",\n",
    "              \"Taylor_series\",\n",
    "              \"Substitution\"]\n",
    "\n",
    "#num_classes = len(topic_tags)\n",
    "\n",
    "def get_all_MER_topics():\n",
    "    '''Returns list of all topics on MER'''\n",
    "    client = MongoClient()\n",
    "    questions_collection = client['merdb'].questions\n",
    "    return questions_collection.find().distinct(\"topics\")\n",
    "\n",
    "def get_questions_with_topics(topics):\n",
    "    '''Returns list of questions with matching topics'''\n",
    "    client = MongoClient()\n",
    "    questions_collection = client['merdb'].questions\n",
    "    if isinstance(topics, basestring):\n",
    "        topics = [topics]\n",
    "    qs = []\n",
    "    for q in questions_collection.find({\"topics\": {\"$in\": topics}}):\n",
    "        qs.append(q)\n",
    "    return qs\n",
    "\n",
    "def count_topics_in_questions(qs):\n",
    "    count_dict = defaultdict(int)\n",
    "    for q in qs:\n",
    "        try:\n",
    "            for topic in q['topics']:\n",
    "                count_dict[topic] += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return count_dict\n",
    "\n",
    "questions_raw = get_questions_with_topics(topic_tags)\n",
    "\n",
    "print('TOTAL:\\n## - Topic\\n==========')\n",
    "for topic, count in count_topics_in_questions(questions_raw).iteritems():\n",
    "    print('%2d - %s' %(count, topic.replace('_', ' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN/TEST:\n",
      "##/## - Topic\n",
      "=============\n",
      "36/11 - Eigenvalues and eigenvectors\n",
      "30/10 - Probability density function\n",
      "38/13 - Taylor series\n",
      "28/ 9 - Substitution\n"
     ]
    }
   ],
   "source": [
    "# for reproducibility we set the seed of the random number generator\n",
    "np.random.seed(23)\n",
    "\n",
    "def train_test_split(qs, TRAIN_PROPORTION=0.75):\n",
    "    '''randomly splits list of questions into two lists for train and test'''\n",
    "    TEST_PROPORTION = 1-TRAIN_PROPORTION\n",
    "    NUM_SAMPLES = int(TEST_PROPORTION * len(qs))\n",
    "    TEST_INDICES = np.random.choice(range(len(qs)), NUM_SAMPLES, replace=False)\n",
    "\n",
    "    qs_train = [q for i, q in enumerate(qs) if not i in TEST_INDICES]\n",
    "    qs_test = [q for i, q in enumerate(qs) if i in TEST_INDICES]\n",
    "    return qs_train, qs_test\n",
    "\n",
    "questions_train, questions_test = train_test_split(questions_raw)\n",
    "\n",
    "print('TRAIN/TEST:\\n##/## - Topic\\n=============')\n",
    "for t in topic_tags:\n",
    "    print('%2d/%2d - %s' % (sum([1 for q in questions_train if t in q['topics']]),\n",
    "                          sum([1 for q in questions_test if t in q['topics']]),\n",
    "                          t.replace('_', ' ')))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def question_to_BOW(q, include_hint_and_sols = True):\n",
    "    '''Transforms a question dictionary q to its bag of words'''\n",
    "    def words_stemmed_no_stop(words):\n",
    "        '''remove commonly used words and combine words with the same root'''\n",
    "        stop = stopwords.words('english')\n",
    "        res = []\n",
    "        for word in words:\n",
    "            stemmed = PorterStemmer().stem_word(word)\n",
    "            if stemmed not in stop and len(stemmed) > 1: #take words longer than 1 char\n",
    "                res.append(stemmed)\n",
    "        return res\n",
    "\n",
    "    all_text = q['statement_html']\n",
    "    if include_hint_and_sols:\n",
    "        for h in q['hints_html']:\n",
    "            all_text += h\n",
    "        for s in q['sols_html']:\n",
    "            all_text += s\n",
    "\n",
    "    all_words = helpers.strip_text(all_text)\n",
    "    bow = words_stemmed_no_stop(all_words)\n",
    "    return ' '.join([w for w in bow])\n",
    "\n",
    "def questions_to_BOW(qs):\n",
    "    '''Transforms list of questions to list of bag of words'''\n",
    "    return [question_to_BOW(q) for q in qs]\n",
    "\n",
    "\n",
    "def question_to_X(q, FILE_TO_LOAD=\"TfidfVectorizer.bin\"):\n",
    "    try:\n",
    "        return vectorizer.transform([question_to_BOW(q)])\n",
    "    except NameError:\n",
    "        vectorizer = pickle.load(open(FILE_TO_LOAD, \"r\"))\n",
    "        return vectorizer.transform([question_to_BOW(q)])\n",
    "\n",
    "    \n",
    "def questions_to_X(qs):\n",
    "    qs_X = [question_to_X(q) for q in qs]\n",
    "    return scipy.sparse.vstack(qs_X)\n",
    "\n",
    "\n",
    "def save_TfidfVectorizer(qs, WHERE_TO_SAVE='TfidfVectorizer.bin'):\n",
    "    '''fits and saves TfidfVectorizer on input list of questions (training set!)'''\n",
    "    vectorizer = TfidfVectorizer(min_df = 2)\n",
    "    vectorizer.fit(questions_to_BOW(qs))\n",
    "    if WHERE_TO_SAVE:\n",
    "        pickle.dump(vectorizer, open(WHERE_TO_SAVE, \"wb\"))\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = save_TfidfVectorizer(questions_train)\n",
    "X_train = questions_to_X(questions_train)\n",
    "X_test = questions_to_X(questions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transform questions into appropriate labels\n",
    "def questions_to_topic_index(qs):\n",
    "    class_indices = range(0, len(topic_tags))\n",
    "    topic_labels = []\n",
    "    for q in qs:\n",
    "        # go through topic_tags, if any of the topics is in the question's topic list\n",
    "        # append its index to topic_labels\n",
    "        for i in class_indices:\n",
    "            if topic_tags[i] in q['topics']:\n",
    "                topic_labels.append(i)\n",
    "                # assumes there is only one topic for each question\n",
    "                break \n",
    "                \n",
    "    return np.asarray(topic_labels)\n",
    "\n",
    "def questions_to_y(qs):\n",
    "    class_indices = range(0, len(topic_tags))\n",
    "    return label_binarize(questions_to_topic_index(qs), class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert X_train.shape[0] == len(questions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = questions_to_y(questions_train)\n",
    "y_test = questions_to_y(questions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SVC for now\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear',\n",
    "                                         probability = True,\n",
    "                                         random_state=np.random.RandomState(0))\n",
    "                                )\n",
    "trained_classifier = classifier.fit(X_train, y_train)\n",
    "pickle.dump(trained_classifier, open(\"svc.bin\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eigenvalues_and_eigenvectors', 'Eigenvalues_and_eigenvectors', 'Eigenvalues_and_eigenvectors', 'Eigenvalues_and_eigenvectors', 'Eigenvalues_and_eigenvectors', 'Eigenvalues_and_eigenvectors', 'Taylor_series', 'Substitution', 'Substitution', 'Substitution', 'Probability_density_function', 'Taylor_series', 'Substitution', 'Probability_density_function', 'Probability_density_function', 'Substitution', 'Taylor_series', 'Substitution', 'Substitution', 'Taylor_series', 'Probability_density_function', 'Substitution', 'Taylor_series', 'Substitution', 'Taylor_series', 'Probability_density_function', 'Probability_density_function', 'Taylor_series', 'Taylor_series', 'Probability_density_function', 'Probability_density_function', 'Taylor_series', 'Probability_density_function', 'Probability_density_function', 'Taylor_series', 'Taylor_series', 'Taylor_series', 'Taylor_series', 'Eigenvalues_and_eigenvectors', 'Eigenvalues_and_eigenvectors', 'Eigenvalues_and_eigenvectors', 'Eigenvalues_and_eigenvectors', 'Eigenvalues_and_eigenvectors']\n"
     ]
    }
   ],
   "source": [
    "def pred_to_topic(pred_array):\n",
    "    return(topic_tags[np.argmax(pred_array)])\n",
    "\n",
    "def preds_to_topics(preds_array):\n",
    "    result = []\n",
    "    for p in preds_array:\n",
    "        result.append(pred_to_topic(p))\n",
    "    return result\n",
    "\n",
    "preds = trained_classifier.predict_proba(X_test)\n",
    "predicted_classes = preds_to_topics(preds)\n",
    "\n",
    "print(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99964 combined micro AUC score.\n"
     ]
    }
   ],
   "source": [
    "def combined_roc_score(correct, predicted):\n",
    "    '''returns micro roc for combined classifier, and dict with roc for all classes'''\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(len(topic_tags)):\n",
    "        fpr[i], tpr[i], _ = roc_curve(correct[:, i], predicted[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(correct.ravel(), predicted.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    return roc_auc[\"micro\"], roc_auc\n",
    "\n",
    "print('%.5f combined micro AUC score.' %combined_roc_score(y_test, preds)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize (todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca.fit(X_train.toarray())\n",
    "pca_X_train = pca.transform(X_train.toarray())\n",
    "pca_X_test = pca.transform(X_test.toarray())\n",
    "print('The first 3 principal components explain %.2f of the variance in the dataset.' % sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#labels_train = [TOPIC1 if _ else TOPIC0 for _ in y_train]\n",
    "#labels_test = [TOPIC1 if _ else TOPIC0 for _ in y_test]\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=25, azim=70)\n",
    "for c, i, label in zip('rgb', class_indices, labels_train):\n",
    "    ax.scatter(pca_X_train[y_train == i, 0],\n",
    "               pca_X_train[y_train == i, 1],\n",
    "               pca_X_train[y_train == i, 2],\n",
    "               c=c, label=label)\n",
    "    \n",
    "    \n",
    "for c, i, label in zip('rgb', [0, 1], [l + ' (test)' for l in labels_test]):\n",
    "    ax.scatter(pca_X_test[y_test == i, 0],\n",
    "               pca_X_test[y_test == i, 1],\n",
    "               pca_X_test[y_test == i, 2],\n",
    "               c=c, label=label, marker='x')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, .5, .4, 1], elev=25, azim=70)\n",
    "\n",
    "y_index_train = questions_to_topic_index(questions_train)\n",
    "\n",
    "print(np.random.rand(num_classes,))\n",
    "\n",
    "for col, i in zip(np.random.rand(num_classes,), range(num_classes)):\n",
    "    print(col)\n",
    "    ax.scatter(pca_X_train[y_index_train==i,0],\n",
    "           pca_X_train[y_index_train==i,1],\n",
    "           pca_X_train[y_index_train==i,2],\n",
    "          c=col, label = topic_tags[i])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "## fix colours (e.g. through random number generator mapped to random cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_topic_for_question(q, classifier, voc):\n",
    "    vec = question_to_vector(q, voc)\n",
    "    pred_prob = classifier.predict_proba(vec)\n",
    "    pred_class = pred_to_topic(pred_prob)\n",
    "    return pred_class\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(predict_topic_for_question(questions[77], trained_classifier, vocabulary_sorted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
