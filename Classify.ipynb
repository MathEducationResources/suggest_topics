{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the topic of a Math Question on Math Education Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **Machine Learning** to predict the topic of a Math Question from the [Math Education Resources](http://math-education-resources.com). For simplicity we will only consider two topics. Using [multiclass classification](https://en.wikipedia.org/wiki/Multiclass_classification) this can be extended to more than two topics (at the time of writing, April 2015, we have about 1500 questions with 150 topics on MER)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do:\n",
    "\n",
    "1. ~~Clean up the code (move helper functions to helper.py) - Bernhard~~\n",
    "2. Fix pca; get feature importance - Alex\n",
    "3. Write convenience functions:\n",
    "  1. text -> topic\n",
    "  2. text -> list of most similar questions (k-nn / cosine dist)  - Alex\n",
    "4. Add the suggested topics to the database for questions w/o a topic\n",
    "5. ~~Re-write code for parent topics - Bernhard~~ -> `question_to_parents` now available\n",
    "6. ~~Re-write train test split to:  - Alex~~\n",
    "    ~~1. get at least one question from each topic~~\n",
    "    ~~2. pick them with diff probabilities~~~~\n",
    "      3. make the code look good / account for errors - Alex\n",
    "7. ROC curve - fix for unbalanced data - Alex\n",
    "8. Put classifier predictions on the website - Alex\n",
    "9. Edit functions to work for both regular and parent topics - Bernhard \n",
    "\n",
    "-----------------------\n",
    "For later:\n",
    "7. Add additional features (course, etc.)\n",
    "    1. graph them\n",
    "8. Put up recommendations on the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import helpers\n",
    "from pymongo import MongoClient\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "\n",
    "# machine learning modules\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL:\n",
      "## - Topic\n",
      "==========\n",
      " 2 - Linear approximation\n",
      " 3 - Power iteration\n",
      " 1 - Alternating series test\n",
      " 1 - Condition number\n",
      " 2 - Implicit differentiation\n",
      " 5 - Matrix diagonalization\n",
      " 1 - Determinants\n",
      " 1 - Center of mass\n",
      " 2 - Partial fractions\n",
      " 1 - Matrix operations\n",
      "45 - Eigenvalues and eigenvectors\n",
      " 1 - Initial value problem\n",
      " 2 - Random walks\n",
      "37 - Substitution\n",
      " 4 - Expected value and median\n",
      " 4 - Trigonometric integral\n",
      " 2 - Rank and nullity\n",
      " 2 - Reflection\n",
      "50 - Taylor series\n",
      " 4 - Cumulative distribution function\n",
      " 8 - Integration by parts\n",
      " 1 - Matrix similarity\n",
      " 2 - Fundamental theorem of calculus\n",
      " 1 - Linear transformation definition\n",
      " 2 - Lagrange interpolation\n",
      "39 - Probability density function\n"
     ]
    }
   ],
   "source": [
    "# create an array of all topics of interest\n",
    "topic_tags = [\"Eigenvalues_and_eigenvectors\",\n",
    "              \"Probability_density_function\",\n",
    "              \"Taylor_series\",\n",
    "              \"Substitution\", \"Lagrange_interpolation\"]\n",
    "\n",
    "\n",
    "questions_raw = helpers.get_questions_with_topics(topic_tags)\n",
    "\n",
    "print('TOTAL:\\n## - Topic\\n==========')\n",
    "for topic, count in helpers.count_topics_in_questions(questions_raw).iteritems():\n",
    "    print('%2d - %s' %(count, topic.replace('_', ' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Integrate'],\n",
       " [u'Integrate'],\n",
       " [u'Integrate'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Integrate'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Integrate'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Integrate'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Integrate'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Integrate'],\n",
       " [u'Integrate'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Integrate'],\n",
       " [u'Integrate'],\n",
       " [u'Integrate'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Integrate'],\n",
       " [u'Integrate'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Integrate'],\n",
       " [u'Integrate'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Integrate', u'Probability_and_statistics'],\n",
       " [u'Integrate'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Integrate'],\n",
       " [u'Integrate'],\n",
       " [u'Integrate', u'Probability_and_statistics'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Differentiate', u'Integrate'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Differentiate', u'Integrate'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Integrate'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Integrate'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Differential_equations', u'Sequences_and_series'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Integrate'],\n",
       " [u'Integrate'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Applications_of_integration', u'Integrate'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Integrate'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Integrate'],\n",
       " [u'Integrate'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Integrate'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Integrate'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Integrate'],\n",
       " [u'Integrate'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Integrate'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Integrate'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Integrate'],\n",
       " [u'Probability_and_statistics'],\n",
       " [u'Differentiate', u'Sequences_and_series'],\n",
       " [u'Differentiate', u'Sequences_and_series'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Differentiate', u'Sequences_and_series'],\n",
       " [u'Differentiate', u'Sequences_and_series'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Sequences_and_series'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra', u'None'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra', u'Probability_and_statistics'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra', u'Probability_and_statistics'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra'],\n",
       " [u'Linear_algebra']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MongoClient()\n",
    "questions_collection = client['merdb'].questions\n",
    "topics_collection = client['merdb'].topics\n",
    "\n",
    "def get_topic_to_parent_dict():\n",
    "    '''returns dict topic -> parent_topic'''\n",
    "    topic_to_parent_dict = dict()\n",
    "    for q in topics_collection.find():\n",
    "        topic_to_parent_dict[q['topic']] = q['parent']\n",
    "    return topic_to_parent_dict\n",
    "\n",
    "topic_to_parent_dict = get_topic_to_parent_dict()\n",
    "\n",
    "def topic_to_parent(topic):\n",
    "    '''returns parent for given topic'''\n",
    "    return topic_to_parent_dict[topic]\n",
    "\n",
    "def question_to_parents(q):\n",
    "    '''returns sorted list of all unique parents of the questions, or [None] if question has no topics or topic is unknown.'''\n",
    "    if not 'topics' in q.keys():\n",
    "        return [None]\n",
    "    parents = []\n",
    "    for topic in q['topics']:\n",
    "        parents.append(topic_to_parent(topic))\n",
    "    return sorted(list(set(parents)))\n",
    "\n",
    "def questions_to_parents(qs):\n",
    "    '''return list of sorted list of all unique parents for all questions.'''\n",
    "    list_of_parents = []\n",
    "    for q in qs:\n",
    "        list_of_parents.append(question_to_parents(q))\n",
    "    return list_of_parents\n",
    "\n",
    "questions_to_parents(questions_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question indices for topic Eigenvalues_and_eigenvectors: \n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171]\n",
      "---------------------------------------------------------------------------\n",
      "Question indices for topic Probability_density_function: \n",
      "[39, 45, 49, 60, 61, 62, 63, 64, 67, 68, 72, 73, 77, 79, 83, 85, 87, 89, 93, 95, 97, 98, 100, 101, 103, 107, 108, 111, 112, 115, 116, 117, 118, 121, 123, 127, 129, 134, 137]\n",
      "---------------------------------------------------------------------------\n",
      "Question indices for topic Taylor_series: \n",
      "[30, 34, 35, 37, 40, 42, 44, 48, 53, 56, 59, 66, 69, 75, 76, 81, 82, 86, 88, 92, 94, 99, 106, 110, 113, 122, 124, 125, 128, 130, 132, 133, 135, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154]\n",
      "---------------------------------------------------------------------------\n",
      "Question indices for topic Substitution: \n",
      "[31, 32, 33, 36, 38, 41, 43, 46, 47, 50, 51, 52, 54, 55, 57, 58, 65, 70, 71, 72, 74, 78, 80, 84, 90, 91, 96, 102, 104, 105, 109, 114, 119, 120, 126, 131, 136]\n",
      "---------------------------------------------------------------------------\n",
      "Question indices for topic Lagrange_interpolation: \n",
      "[0, 1]\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def question_indices_by_topic(qs):\n",
    "    '''Returns the list of len(topic_tags) containing a list of question indices for each topic'''\n",
    "    all_indices = [[] for i in range(len(topic_tags))]\n",
    "    for i, q in enumerate(qs):\n",
    "        for j, t in enumerate(topic_tags):\n",
    "            if t in q['topics']:\n",
    "                all_indices[j].append(i)\n",
    "    return all_indices\n",
    "                \n",
    "\n",
    "for i, topic in enumerate(topic_tags):\n",
    "    print \"Question indices for topic %s: \\n\" % topic, question_indices_by_topic(questions_raw)[i]\n",
    "    print \"---------------------------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions picked: [21, 137, 145, 50, 0]\n"
     ]
    }
   ],
   "source": [
    "# for reproducibility we set the seed of the random number generator\n",
    "np.random.seed(23)\n",
    "\n",
    "def pick_random_index_per_topic(qs):\n",
    "    '''Returns a list of randomly chosen question indices - one for each topic'''\n",
    "    question_indices = question_indices_by_topic(qs)\n",
    "    result = []\n",
    "    for indices in question_indices:\n",
    "        # pick random index\n",
    "        question_index_for_topic = np.random.choice(indices)\n",
    "        # add to result list, avoiding duplicates in case questions match more than one topic\n",
    "        if question_index_for_topic not in result:\n",
    "            result.append(question_index_for_topic)\n",
    "    return result\n",
    "\n",
    "print \"Questions picked:\", pick_random_index_per_topic(questions_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4], [4], [], [5]]\n"
     ]
    }
   ],
   "source": [
    "def remove_from_question_indices(ls, indices_by_topic):\n",
    "    '''Takes a list ls and a list of lists indices_by_topic\n",
    "        and removes all elements of ls from each element of indices_by_topic'''\n",
    "    \n",
    "    for index_list in indices_by_topic:\n",
    "        for element in ls:\n",
    "            if element in index_list:\n",
    "                index_list.remove(element)\n",
    "    \n",
    "    return indices_by_topic\n",
    "\n",
    "#example\n",
    "print(remove_from_question_indices([1, 2, 3], [[1, 2, 4],[4, 3],[],[5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper functions for the train_test_split function\n",
    "\n",
    "def sample_from_all_classes(indices_by_topic, num_total_samples, num_questions):\n",
    "    if (num_total_samples <= 0):\n",
    "        return []\n",
    "    \n",
    "    sample_indices = set([])\n",
    "    for index_list in indices_by_topic:\n",
    "        class_proportion = float(len(index_list)) / num_questions\n",
    "        num_class_samples = int(num_total_samples * class_proportion)\n",
    "        class_samples = sample_from_class(index_list, num_class_samples)\n",
    "        # update the set sample_indices with new class samples\n",
    "        sample_indices.update(class_samples)\n",
    "    return list(sample_indices)\n",
    "\n",
    "def sample_from_class(indices, n):\n",
    "    return np.random.choice(indices, n, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN/TEST:\n",
      "##/## - Topic\n",
      "=============\n",
      "33/12 - Eigenvalues and eigenvectors\n",
      "29/10 - Probability density function\n",
      "37/13 - Taylor series\n",
      "27/10 - Substitution\n",
      " 1/ 1 - Lagrange interpolation\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(23)\n",
    "\n",
    "def train_test_split(qs, TRAIN_PROPORTION=0.75):\n",
    "    '''randomly splits list of questions into two lists for train and test'''\n",
    "    TRAIN_SIZE = int(TRAIN_PROPORTION * len(qs))\n",
    "                        \n",
    "    # pick a question from each topic and add to training set\n",
    "    indices_from_each_topic = pick_random_index_per_topic(qs)\n",
    "                        \n",
    "    # from the rest of the questions, pick indices from each class according to topic probabilities:\n",
    "    indices_left = remove_from_question_indices(indices_from_each_topic, question_indices_by_topic(qs))\n",
    "    samples_left_to_take = TRAIN_SIZE - len(indices_from_each_topic)\n",
    "    \n",
    "    randomly_picked_indices = sample_from_all_classes(indices_left, \n",
    "                                                    samples_left_to_take, \n",
    "                                                     len(qs)-len(indices_from_each_topic))\n",
    "    \n",
    "    train_indices = indices_from_each_topic + randomly_picked_indices\n",
    "    \n",
    "   \n",
    "    qs_train = [q for i, q in enumerate(qs) if i in train_indices]\n",
    "    qs_test = [q for i, q in enumerate(qs) if not i in train_indices]\n",
    "    \n",
    "    permuted = np.random.permutation(len(qs_train))\n",
    "    qs_train_permuted = [qs_train[i] for i in permuted]\n",
    "    return qs_train_permuted, qs_test\n",
    "\n",
    "questions_train, questions_test = train_test_split(questions_raw)\n",
    "\n",
    "print('TRAIN/TEST:\\n##/## - Topic\\n=============')\n",
    "for t in topic_tags:\n",
    "    print('%2d/%2d - %s' % (sum([1 for q in questions_train if t in q['topics']]),\n",
    "                          sum([1 for q in questions_test if t in q['topics']]),\n",
    "                          t.replace('_', ' ')))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = helpers.save_TfidfVectorizer(questions_train)\n",
    "\n",
    "X_train = helpers.questions_to_X(questions_train)\n",
    "X_test = helpers.questions_to_X(questions_test)\n",
    "assert X_train.shape[0] == len(questions_train)\n",
    "\n",
    "y_train = helpers.questions_to_y(questions_train, topic_tags)\n",
    "y_test = helpers.questions_to_y(questions_test, topic_tags)\n",
    "assert len(y_train) == len(questions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SVC for now\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear',\n",
    "                                         probability = True,\n",
    "                                         random_state=np.random.RandomState(0))\n",
    "                                )\n",
    "trained_classifier = classifier.fit(X_train, y_train)\n",
    "pickle.dump(trained_classifier, open(\"svc.bin\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taylor_series\n",
      "Eigenvalues_and_eigenvectors\n",
      "Eigenvalues_and_eigenvectors\n",
      "Eigenvalues_and_eigenvectors\n",
      "Eigenvalues_and_eigenvectors\n",
      "Eigenvalues_and_eigenvectors\n",
      "Eigenvalues_and_eigenvectors\n",
      "Eigenvalues_and_eigenvectors\n",
      "Substitution\n",
      "Substitution\n",
      "Probability_density_function\n",
      "Taylor_series\n",
      "Substitution\n",
      "Probability_density_function\n",
      "Substitution\n",
      "Probability_density_function\n",
      "Substitution\n",
      "Substitution\n",
      "Substitution\n",
      "Probability_density_function\n",
      "Substitution\n",
      "Taylor_series\n",
      "Substitution\n",
      "Probability_density_function\n",
      "Probability_density_function\n",
      "Taylor_series\n",
      "Probability_density_function\n",
      "Probability_density_function\n",
      "Probability_density_function\n",
      "Probability_density_function\n",
      "Taylor_series\n",
      "Taylor_series\n",
      "Substitution\n",
      "Taylor_series\n",
      "Taylor_series\n",
      "Taylor_series\n",
      "Taylor_series\n",
      "Taylor_series\n",
      "Taylor_series\n",
      "Taylor_series\n",
      "Taylor_series\n",
      "Eigenvalues_and_eigenvectors\n",
      "Eigenvalues_and_eigenvectors\n",
      "Eigenvalues_and_eigenvectors\n",
      "Eigenvalues_and_eigenvectors\n",
      "Eigenvalues_and_eigenvectors\n"
     ]
    }
   ],
   "source": [
    "preds = trained_classifier.predict_proba(X_test)\n",
    "predicted_classes = helpers.preds_to_topics(preds, topic_tags)\n",
    "\n",
    "print('\\n'.join(predicted_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found arrays with inconsistent numbers of samples: [45 46]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-e9faba73133a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%.5f combined micro AUC score.'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[0mhelpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombined_roc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/alexf/suggest_topics/helpers.pyc\u001b[0m in \u001b[0;36mcombined_roc_score\u001b[1;34m(correct, predicted)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[0mroc_auc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0mfpr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m         \u001b[0mroc_auc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;31m# Compute micro-average ROC curve and ROC area\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alexf/.local/lib/python2.7/site-packages/sklearn/metrics/ranking.pyc\u001b[0m in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    475\u001b[0m     \"\"\"\n\u001b[0;32m    476\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[1;32m--> 477\u001b[1;33m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alexf/.local/lib/python2.7/site-packages/sklearn/metrics/ranking.pyc\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[0mDecreasing\u001b[0m \u001b[0mscore\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m     \"\"\"\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alexf/.local/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         raise ValueError(\"Found arrays with inconsistent numbers of samples: \"\n\u001b[1;32m--> 174\u001b[1;33m                          \"%s\" % str(uniques))\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found arrays with inconsistent numbers of samples: [45 46]"
     ]
    }
   ],
   "source": [
    "print('%.5f combined micro AUC score.' %helpers.combined_roc_score(y_test, preds)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize (todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 3 principal components explain 0.20 of the variance in the dataset.\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca.fit(X_train.toarray())\n",
    "pca_X_train = pca.transform(X_train.toarray())\n",
    "pca_X_test = pca.transform(X_test.toarray())\n",
    "print('The first 3 principal components explain %.2f of the variance in the dataset.' % sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#labels_train = [TOPIC1 if _ else TOPIC0 for _ in y_train]\n",
    "#labels_test = [TOPIC1 if _ else TOPIC0 for _ in y_test]\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=25, azim=70)\n",
    "for c, i, label in zip('rgb', class_indices, labels_train):\n",
    "    ax.scatter(pca_X_train[y_train == i, 0],\n",
    "               pca_X_train[y_train == i, 1],\n",
    "               pca_X_train[y_train == i, 2],\n",
    "               c=c, label=label)\n",
    "    \n",
    "    \n",
    "for c, i, label in zip('rgb', [0, 1], [l + ' (test)' for l in labels_test]):\n",
    "    ax.scatter(pca_X_test[y_test == i, 0],\n",
    "               pca_X_test[y_test == i, 1],\n",
    "               pca_X_test[y_test == i, 2],\n",
    "               c=c, label=label, marker='x')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, .5, .4, 1], elev=25, azim=70)\n",
    "\n",
    "y_index_train = questions_to_topic_index(questions_train)\n",
    "\n",
    "print(np.random.rand(num_classes,))\n",
    "\n",
    "for col, i in zip(np.random.rand(num_classes,), range(num_classes)):\n",
    "    print(col)\n",
    "    ax.scatter(pca_X_train[y_index_train==i,0],\n",
    "           pca_X_train[y_index_train==i,1],\n",
    "           pca_X_train[y_index_train==i,2],\n",
    "          c=col, label = topic_tags[i])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "## fix colours (e.g. through random number generator mapped to random cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_topic_for_question(q, classifier, voc):\n",
    "    vec = question_to_vector(q, voc)\n",
    "    pred_prob = classifier.predict_proba(vec)\n",
    "    pred_class = pred_to_topic(pred_prob)\n",
    "    return pred_class\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'questions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-771592ca2c36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_topic_for_question\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m77\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrained_classifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'questions' is not defined"
     ]
    }
   ],
   "source": [
    "print(predict_topic_for_question(questions_raw[77], trained_classifier, vocabulary_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
